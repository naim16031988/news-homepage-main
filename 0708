\chapter{Description of the Evaluation Environment}

\section{Introduction}
This chapter presents the evaluation environment used to test and validate our approach. 
Describing the infrastructure and software stack is essential to ensure that the experiments 
can be reproduced and that the results are reliable. The environment includes both the 
hardware resources and the software components, as well as the workloads and monitoring 
tools employed during the evaluation. 

\section{Hardware Infrastructure}
The experiments were conducted on a small-scale cloud environment consisting of several 
servers. Each server is equipped with a multi-core processor, a fixed amount of RAM, and 
local storage. The servers are interconnected through a high-speed network to ensure low 
latency and efficient communication. 
When virtualization is used, each server hosts multiple virtual machines (VMs), configured 
with predefined CPU and memory allocations. This setup reflects the typical heterogeneity 
found in modern cloud data centers.

\section{Software Environment}
The software stack combines operating systems, containerization platforms, and monitoring 
tools. All machines run a Linux-based operating system with a recent kernel version. 
Containerization is managed through Docker, and in some experiments, Kubernetes is used 
for orchestration. 
To monitor the system and collect resource usage metrics, we rely on tools such as 
Prometheus, Node Exporter, and cAdvisor. For energy monitoring, the Scaphandre framework 
is employed, as it allows us to capture host- and VM-level power consumption.

\section{Workloads and Applications}
To evaluate the proposed solution, we deploy representative workloads. These include 
synthetic benchmarks that generate controlled CPU and memory usage, as well as realistic 
applications such as machine learning training jobs and network-intensive services. 
The workloads are chosen to create different stress levels on the infrastructure and to 
simulate realistic cloud scenarios. In addition, Service-Level Agreement (SLA) constraints 
are considered to reflect the requirements of real customers.

\section{Metrics and Data Collection}
The evaluation relies on two categories of metrics. The first category concerns system-level 
metrics, such as CPU utilization, memory usage, disk I/O, and network throughput. These 
are collected through Prometheus and its exporters. The second category concerns energy 
metrics, obtained from Scaphandre, which reports power consumption at the server or VM 
level. All metrics are stored in a time-series database and visualized through Grafana 
dashboards for further analysis.

\section{Experimental Setup}
The experiments follow a reproducible methodology. The workloads are deployed in the 
evaluation environment following a defined topology, where multiple VMs or containers are 
scheduled across the servers. Each experiment is repeated multiple times to reduce bias and 
to validate the stability of the results. Data is collected continuously during the execution of 
the workloads and analyzed afterward. 
The main assumptions and limitations of this setup are that the environment is smaller than 
a real cloud data center, and that the results may vary when scaled to a larger infrastructure. 
Nevertheless, the chosen setup is sufficient to demonstrate the feasibility and effectiveness 
of the proposed approach.

\section{Summary}
In summary, the evaluation environment combines physical servers, virtualization, and 
modern monitoring tools to create a realistic yet controlled setup. It provides the necessary 
infrastructure to test resource utilization, SLA compliance, and energy consumption. This 
environment is therefore well suited for validating our contribution.








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score

from tensorflow.keras.models import load_model

# -----------------------------
# Load your new dataset
# -----------------------------
df = pd.read_csv("new_dataset.csv")  # Your second dataset
cpu_data = df['cpu'].values.reshape(-1, 1)

# -----------------------------
# Scale the data
# -----------------------------
scaler = MinMaxScaler()
cpu_scaled = scaler.fit_transform(cpu_data)

# -----------------------------
# Reuse model trained earlier
# -----------------------------
model = load_model("lstm_cpu_model.h5")  # Load your trained model

# -----------------------------
# Create sequences for 3 min ahead
# -----------------------------
def create_sequences(data, window_size, horizon):
    X, y = [], []
    for i in range(len(data) - window_size - horizon):
        X.append(data[i:i + window_size])
        y.append(data[i + window_size + horizon - 1])  # Predict t + 9
    return np.array(X), np.array(y)

window_size = 10
horizon = 9

X_new, y_new = create_sequences(cpu_scaled, window_size, horizon)

# -----------------------------
# Predict with the pre-trained model
# -----------------------------
y_pred_scaled = model.predict(X_new)
y_pred = scaler.inverse_transform(y_pred_scaled).flatten()
y_true = scaler.inverse_transform(y_new).flatten()

# -----------------------------
# Evaluation
# -----------------------------
mape = mean_absolute_percentage_error(y_true, y_pred) * 100
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
r2 = r2_score(y_true, y_pred)

print(f"MAPE on new data: {mape:.2f}%")
print(f"RMSE on new data: {rmse:.4f}")
print(f"RÂ² Score on new data: {r2:.4f}")

# -----------------------------
# Plot predictions
# -----------------------------
cpu_full = scaler.inverse_transform(cpu_scaled).flatten()
y_pred_plot = np.empty_like(cpu_full)
y_pred_plot[:] = np.nan
start_index = window_size + horizon - 1
y_pred_plot[start_index:start_index + len(y_pred)] = y_pred

plt.figure(figsize=(14, 6))
plt.plot(cpu_full, label="Actual CPU")
plt.plot(y_pred_plot, label="Predicted CPU (3min ahead)", linestyle="--")
plt.title("Using Pre-Trained Model on New Dataset")
plt.xlabel("Time Step")
plt.ylabel("CPU Usage")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# -----------------------------
# Load new dataset
# -----------------------------
df = pd.read_csv("your_new_dataset.csv")  # replace with your file
cpu_data = df['cpu'].values.reshape(-1, 1)  # column must be named 'cpu'

# -----------------------------
# Preprocessing
# -----------------------------
scaler = MinMaxScaler()
cpu_scaled = scaler.fit_transform(cpu_data)

# -----------------------------
# Create sequences for 3 min ahead (9 steps)
# -----------------------------
def create_sequences(data, window_size, horizon):
    X, y = [], []
    for i in range(len(data) - window_size - horizon):
        X.append(data[i:i + window_size])
        y.append(data[i + window_size + horizon - 1])  # predict at t + horizon
    return np.array(X), np.array(y)

window_size = 10  # number of past steps to use
horizon = 9       # predict 9 steps = 3 min ahead

X, y = create_sequences(cpu_scaled, window_size, horizon)

# -----------------------------
# Train/test split
# -----------------------------
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# -----------------------------
# Train LSTM
# -----------------------------
model = Sequential([
    LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=20, batch_size=8, verbose=0)

# -----------------------------
# Predict for full dataset
# -----------------------------
X_full, y_full = create_sequences(cpu_scaled, window_size, horizon)
y_pred = model.predict(X_full)

# Inverse transform
y_pred_inv = scaler.inverse_transform(y_pred).flatten()
y_full_inv = scaler.inverse_transform(y_full).flatten()

# -----------------------------
# Evaluation
# -----------------------------
mape = mean_absolute_percentage_error(y_full_inv, y_pred_inv) * 100
rmse = np.sqrt(mean_squared_error(y_full_inv, y_pred_inv))
r2 = r2_score(y_full_inv, y_pred_inv)

print(f"MAPE (3 min ahead): {mape:.2f}%")
print(f"RMSE: {rmse:.4f}")
print(f"RÂ²: {r2:.4f}")

# -----------------------------
# Plot actual vs predicted
# -----------------------------
cpu_full = scaler.inverse_transform(cpu_scaled).flatten()

# Align predicted values with full CPU timeline
y_pred_plot = np.empty_like(cpu_full)
y_pred_plot[:] = np.nan
start_index = window_size + horizon - 1
y_pred_plot[start_index:start_index + len(y_pred_inv)] = y_pred_inv

plt.figure(figsize=(14, 6))
plt.plot(cpu_full, label='Actual CPU')
plt.plot(y_pred_plot, label='Predicted CPU (3 min ahead)', linestyle='--')
plt.title("LSTM CPU Prediction 3 Minutes Ahead")
plt.xlabel("Time Step")
plt.ylabel("CPU Usage")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()








# --- Predict on full dataset ---
X_full, y_full = create_sequences(cpu_scaled, window_size)
y_full_pred_scaled = model.predict(X_full)

# Inverse transform predictions and true values
y_full_pred = scaler.inverse_transform(y_full_pred_scaled)
y_full_actual = scaler.inverse_transform(y_full)

# Flatten for metrics
y_actual_flat = y_full_actual.flatten()
y_pred_flat = y_full_pred.flatten()

# --- Compute metrics on full dataset ---
mape = mean_absolute_percentage_error(y_actual_flat, y_pred_flat) * 100
rmse = np.sqrt(mean_squared_error(y_actual_flat, y_pred_flat))
r2 = r2_score(y_actual_flat, y_pred_flat)

print(f"MAPE (Full): {mape:.2f}%")
print(f"RMSE (Full): {rmse:.4f}")
print(f"RÂ² Score (Full): {r2:.4f}")

# --- Build prediction timeline to match full CPU series ---
cpu_full = scaler.inverse_transform(cpu_scaled).flatten()
y_pred_full = np.empty_like(cpu_full)
y_pred_full[:] = np.nan
y_pred_full[window_size:] = y_pred_flat  # offset due to sequence window

# --- Plot full predictions ---
plt.figure(figsize=(14, 6))
plt.plot(cpu_full, label='Original CPU')
plt.plot(y_pred_full, label='Predicted CPU (Train + Test)', linestyle='--')
plt.title("LSTM CPU Prediction on Full Dataset")
plt.xlabel("Time")
plt.ylabel("CPU Usage")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()









@app.route('/scale_down', methods=['POST'])
def scale_down():
    global previous_state, previous_action, previous_action_time

    container_counts = get_container_count()
    current_state = build_state(container_counts)
    now = time.time()
    now_str = datetime.fromtimestamp(now).strftime("%Y-%m-%d %H:%M:%S")

    if previous_state is not None and previous_action is not None and previous_action_time is not None:
        start_str = datetime.fromtimestamp(previous_action_time).strftime("%Y-%m-%d %H:%M:%S")
        duration = now - previous_action_time
        power = normalize(query_power(), 0, 170)
        latency = normalize(query_latency(), 0, 180)
        fps = normalize(query_fps(), 0, 12)
        reward = -2 * power + 3 * latency + 4 * fps

        print(f"[Scale Down] Reward calculation from {start_str} to {now_str} (duration: {duration:.2f} seconds), state: {previous_state}, action: {previous_action}, power: {power:.2f}, latency: {latency:.2f}, fps: {fps:.2f}")

        dqn_agent.remember(previous_state, previous_action, reward, current_state)
        dqn_agent.replay()

        # Reset tracking to pause learning until next scale-up
        previous_state = None
        previous_action = None
        previous_action_time = None

    return jsonify({"message": "Scale-down completed and reward computed."})import requests
import statistics

# Replace with your values
PROM_URL = "http://localhost:9090"
INSTANCE = "vm1:9100"
START = "2025-08-06T12:00:00Z"
END = "2025-08-06T13:00:00Z"
STEP = "30s"

# Query URL
url = f"{PROM_URL}/api/v1/query_range"
params = {
    "query": f'scaph_host_power_microwatt{{instance="{INSTANCE}"}}',
    "start": START,
    "end": END,
    "step": STEP
}

# Send request
response = requests.get(url, params=params)
data = response.json()

# Extract and average values
if data["status"] == "success":
    values = data["data"]["result"][0]["values"]
    watts = [float(val) / 1_000_000 for _, val in values]
    average_power = statistics.mean(watts)
    print(f"Average Power: {average_power:.6f} Watts")
else:
    print("Query failed:", data)








def create_container(destination_ip, agent_name, image_name, port, has_gpu):
    DOCKER_HOST = f"http://{destination_ip}:2375"
    unique_id = uuid.uuid4()
    container_name = f"{agent_name}_{unique_id}"
    DEFAULT_DISPLAY = os.environ.get("DISPLAY", ":0")
    XAUTH = os.path.expanduser("~/.Xauthority")
    url = f"{DOCKER_HOST}/containers/create?name={container_name}"
    headers = {"Content-Type": "application/json"}

    env_vars = [
        f"DISPLAY={DEFAULT_DISPLAY}",
        f"PORT={port}",
        "XAUTHORITY=/root/.Xauthority"
    ]

    host_config = {
        "NetworkMode": "host",
        "NanoCpus": 5000000000,
        "Binds": [
            "/tmp/.X11-unix:/tmp/.X11-unix",
            f"{XAUTH}:/root/.Xauthority"
        ]
    }

    if has_gpu == 1:
        host_config["DeviceRequests"] = [{
            "Driver": "nvidia",
            "Count": -1,
            "Capabilities": [["gpu"]],
            "Options": {}
        }]
        env_vars += [
            "NVIDIA_VISIBLE_DEVICES=all",
            "NVIDIA_DRIVER_CAPABILITIES=compute,utility"
        ]

    config = {
        "Image": image_name,
        "Env": env_vars,
        "HostConfig": host_config
    }

    res = requests.post(url, headers=headers, data=json.dumps(config))
    if res.status_code != 201:
        print(f"[ERROR] Failed to create container on {destination_ip}: {res.text}")
        return None

    container_id = res.json()["Id"]
    print(f"Container created: {container_id}")

    start_url = f"{DOCKER_HOST}/containers/{container_id}/start"
    start = requests.post(start_url)
    if start.status_code == 204:
        print("Container started")
    else:
        print("Failed to start container:", start.text)









from flask import Flask, jsonify
import requests
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
import time
import datetime

app = Flask(__name__)

# ---- Config ----
PROMETHEUS_URL = "http://prometheus:9090"  # Use internal Docker name if on the same network
CONTAINER_NAME = "agent"                   # Replace with your container name
STEP = 20                                  # seconds between Prometheus samples
POINTS = 10                                # number of points to fetch

def fetch_cpu_usage():
    now = int(time.time())
    start = now - (POINTS * STEP)

    query = f'container_cpu_usage_seconds_total{{container="{CONTAINER_NAME}"}}'

    response = requests.get(
        f"{PROMETHEUS_URL}/api/v1/query_range",
        params={
            "query": f"rate({query}[20s])",  # Smooth with rate over 20s
            "start": start,
            "end": now,
            "step": f"{STEP}s"
        }
    )

    result = response.json().get("data", {}).get("result", [])
    if not result or not result[0]["values"]:
        return []

    values = result[0]["values"]
    return [float(v[1]) for v in values]

@app.route("/predict", methods=["GET"])
def predict_cpu():
    cpu_data = fetch_cpu_usage()

    if len(cpu_data) < 10:
        return jsonify({"error": "Not enough CPU data points"}), 400

    try:
        model = ARIMA(cpu_data, order=(3, 0, 0))
        model_fit = model.fit()
        forecast = model_fit.forecast(steps=12)  # 12 * 15s = 3 minutes
        prediction = forecast.tolist()[-1]
        return jsonify({
            "prediction": prediction,
            "unit": "CPU (0-1 scale)",
            "horizon": "3 minutes"
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)



# 1. Upload your CSV
from google.colab import files
uploaded = files.upload()

# 2. Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from math import sqrt

# 3. Load and preprocess
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)
df['timestamp'] = pd.to_datetime(df['timestamp'])
df = df.sort_values('timestamp')
df = df.set_index('timestamp')
df = df.asfreq('20S')  # adjust if different
df['cpu'] = df['cpu'].astype(float)

# 4. ARMA rolling forecast evaluation
def evaluate_arma_over_windows(df, max_window_size=30, prediction_horizon=180):
    results = []
    for window_size in range(3, max_window_size + 1):
        y_true, y_pred = [], []
        for i in range(window_size, len(df) - prediction_horizon):
            train_series = df['cpu'].iloc[i - window_size:i]
            try:
                model = ARIMA(train_series, order=(3, 0, 0))
                model_fit = model.fit()
                forecast = model_fit.forecast(steps=prediction_horizon)
                pred = forecast[-1]
                actual = df['cpu'].iloc[i + prediction_horizon]
                y_true.append(actual)
                y_pred.append(pred)
            except:
                continue
        if y_true:
            mape = np.mean(np.abs((np.array(y_true) - np.array(y_pred)) / np.maximum(np.abs(y_true), 1e-5))) * 100
            rmse = sqrt(mean_squared_error(y_true, y_pred))
            r2 = r2_score(y_true, y_pred)
            results.append({
                "window_size": window_size,
                "MAPE": mape,
                "RMSE": rmse,
                "R2": r2
            })
    return pd.DataFrame(results)

# 5. Plot for different horizons
horizons = [60, 120, 180, 240, 300, 360]
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))
axes = axes.flatten()

for idx, horizon in enumerate(horizons):
    print(f"Evaluating for horizon: {horizon} sec")
    results_df = evaluate_arma_over_windows(df, max_window_size=30, prediction_horizon=horizon // 20)
    ax = axes[idx]
    ax.plot(results_df['window_size'], results_df['MAPE'], label='MAPE', marker='o')
    ax.plot(results_df['window_size'], results_df['RMSE'], label='RMSE', marker='o')
    ax.plot(results_df['window_size'], results_df['R2'], label='RÂ² Score', marker='o')
    ax.set_title(f'Horizon = {horizon}s')
    ax.set_xlabel("Window Size")
    ax.set_ylabel("Error / Score")
    ax.grid(True)
    ax.legend()

plt.tight_layout()
plt.show()o









import paramiko

def configure_docker_remote_api(host, username, key_path=None, password=None):
    commands = [
        # 1. Create directory
        "sudo mkdir -p /etc/systemd/system/docker.service.d",
        # 2. Create override.conf with the required ExecStart
        """echo -e '[Service]\\nExecStart=\\nExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375' | sudo tee /etc/systemd/system/docker.service.d/override.conf""",
        # 3. Reload systemd and restart Docker
        "sudo systemctl daemon-reexec",
        "sudo systemctl daemon-reload",
        "sudo systemctl restart docker"
    ]

    print(f"ðŸ” Connecting to {host}...")
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    
    try:
        if key_path:
            ssh.connect(hostname=host, username=username, key_filename=key_path)
        else:
            ssh.connect(hostname=host, username=username, password=password)
        
        for cmd in commands:
            print(f"ðŸ› ï¸ Executing: {cmd}")
            stdin, stdout, stderr = ssh.exec_command(cmd)
            exit_code = stdout.channel.recv_exit_status()
            if exit_code == 0:
                print(stdout.read().decode().strip())
            else:
                print(f"âŒ Error in: {cmd}\n{stderr.read().decode()}")
                break

        print("âœ… Docker API configured on port 2375 (insecure)")
    finally:
        ssh.close()

# Example usage
if __name__ == "__main__":
    configure_docker_remote_api(
        host="192.168.122.101",    # ðŸ” Replace with your VM IP
        username="your_user",      # ðŸ” Replace with remote username
        key_path="/path/to/id_rsa" # or use password="your_password"
    )



import os
import requests
import json

DOCKER_HOST = "http://192.168.122.101:2375"
DEFAULT_DISPLAY = os.environ.get("DISPLAY", ":0")
XAUTH = os.path.expanduser("~/.Xauthority")

def create_container(container_name, image_name, port):
    url = f"{DOCKER_HOST}/containers/create?name={container_name}"
    headers = {"Content-Type": "application/json"}

    config = {
        "Image": image_name,
        "Env": [
            f"DISPLAY={DEFAULT_DISPLAY}",
            f"PORT={port}",
            "XAUTHORITY=/root/.Xauthority"
        ],
        "HostConfig": {
            "NetworkMode": "host",
            "NanoCpus": 3000000000,  # 3 CPUs = 3 * 1e9 nanocpus
            "Binds": [
                "/tmp/.X11-unix:/tmp/.X11-unix",
                f"{XAUTH}:/root/.Xauthority"
            ]
        }
    }

    # 1. CrÃ©ation du conteneur
    res = requests.post(url, headers=headers, data=json.dumps(config))
    if res.status_code != 201:
        print("âŒ Failed to create container:", res.text)
        return None
    container_id = res.json()["Id"]
    print(f"âœ… Container created: {container_id}")

    # 2. DÃ©marrage du conteneur
    start_url = f"{DOCKER_HOST}/containers/{container_id}/start"
    start = requests.post(start_url)
    if start.status_code == 204:
        print("ðŸš€ Container started")
    else:
        print("âŒ Failed to start container:", start.text)

if __name__ == "__main__":
    create_container(
        container_name="my_app",
        image_name="your_image_name",
        port="8081"
    )




@app.route('/get_vm_to_scale_down', methods=['GET'])
def get_vm_to_scale_down():
    global last_state, last_action, last_energy, last_time

    container_counts = get_container_count(prometheus_url)
    max_index = container_counts.index(max(container_counts))
    vm_ip_to_remove = vm_ips[max_index]

    # Update the container count BEFORE computing next state
    container_counts[max_index] -= 1
    next_state = build_state(container_counts)

    if last_state is not None and last_action is not None and last_energy is not None and last_time is not None:
        current_energy = get_average_power_from_hosts(host_ips)
        current_time = time.time()
        delta_energy = current_energy - last_energy
        delta_time = current_time - last_time

        if delta_time != 0:
            power_watts = delta_energy / (delta_time * 1_000_000)
            reward = -power_watts
            print("Updating Q-table for scale-down")
            update_q_table(last_state, last_action, reward, next_state, len(container_counts))

    print(f"return this: {vm_ip_to_remove}")
    return jsonify({
        "scaled_down_vm_ip": vm_ip_to_remove,
        "message": "Selected most saturated VM"
    })






max_index = c





ontainer_counts.index(max(container_counts))
vm_ip_to_remove = vm_ips[max_index]

container_counts[max_index] -= 1  # MUST be done before computing next_state
next_state = build_state(container_counts)

if last_state is not None  last_action is not None and last_energy is not None and last_time is not None:
    current_energy = get_average_power_from_hosts(host_ips)
    current_time = time.time()
    delta_energy = current_energy - last_energy
    delta_time = current_time - last_time
    if delta_time > 0:
        power_watts = delta_energy / (delta_time * 1_000_000)
        reward = -power_watts
        print("Updating Q-table for scale-down")
        update_q_table(last_state, last_action, reward, next_state, len(container_counts))



import zmq
import cv2
import numpy as np
import threading

# List of ports your detectors are sending on
DETECTOR_PORTS = [5557, 5558]

# Store latest frame from any detector
latest_frame = None
lock = threading.Lock()

def receive_from_detector(port):
    global latest_frame
    context = zmq.Context()
    socket = context.socket(zmq.PULL)
    socket.connect(f"tcp://localhost:{port}")
    print(f"[Viewer] Connected to detector on port {port}")

    while True:
        try:
            frame_bytes = socket.recv()
            npimg = np.frombuffer(frame_bytes, dtype=np.uint8)
            frame = cv2.imdecode(npimg, 1)

            with lock:
                latest_frame = frame

        except Exception as e:
            print(f"[Viewer] Error receiving from port {port}: {e}")

# Start a thread for each detector connection
for port in DETECTOR_PORTS:
    t = threading.Thread(target=receive_from_detector, args=(port,), daemon=True)
    t.start()

# Display loop
cv2.namedWindow("Unified Viewer", cv2.WINDOW_NORMAL)
cv2.resizeWindow("Unified Viewer", 960, 540)

while True:
    with lock:
        if latest_frame is not None:
            cv2.imshow("Unified Viewer", latest_frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cv2.destroyAllWindows()
# Global writer, initialized only once
video_writer = None
output_path = "annotated_output.avi"
fourcc = cv2.VideoWriter_fourcc(*'XVID')
fps = 20  # You can adjust this

cv2.namedWindow("YOLOv5 Reconstructed", cv2.WINDOW_NORMAL)
cv2.resizeWindow("YOLOv5 Reconstructed", 960, 540)

while True:
    if expected_frame_id in frame_buffer:
        frame = frame_buffer.pop(expected_frame_id)

        # Initialize writer when first frame arrives
        if video_writer is None:
            h, w = frame.shape[:2]
            video_writer = cv2.VideoWriter(output_path, fourcc, fps, (w, h))
            print(f"[INFO] Saving video to: {output_path}")

        # Write frame to video file
        video_writer.write(frame)

        # Display frame
        cv2.imshow("YOLOv5 Reconstructed", frame)
        expected_frame_id += 1

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Clean up
if video_writer:
    video_writer.release()
cv2.destroyAllWindows()





import cv2

video_path = "annotated_output.avi"  # or .mp4 depending on your output

cap = cv2.VideoCapture(video_path)

if not cap.isOpened():
    print("âŒ Could not open video file.")
    exit()

while True:
    ret, frame = cap.read()
    if not ret:
        break

    cv2.imshow("Saved Annotated Video", frame)

    if cv2.waitKey(25) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()










def receive_from_vm(ip, port, stop_event):
    sock = context.socket(zmq.PULL)
    sock.connect(f"tcp://{ip}:{port}")
    print(f"[INFO] Connected to detector at {ip}:{port}")
    
    while not stop_event.is_set():
        try:
            msg = sock.recv(flags=zmq.NOBLOCK)
            data = msgpack.unpackb(msg, raw=False)
            frame_id = data["frame_id"]
            jpg = data["image"]
            print(f"frame id :{frame_id} with size: {len(msg)} received from VM {ip}")
            frame = cv2.imdecode(np.frombuffer(jpg, np.uint8), cv2.IMREAD_COLOR)
            if frame is not None:
                frame_buffer[frame_id] = frame
        except zmq.Again:
            time.sleep(0.01)
        except Exception as e:
            print(f"[ERROR] {ip}:{port} {e}")
            break

    print(f"[INFO] Stopped receiver for {ip}:{port}")
    sock.close()
    cv2.destroyWindow(f"{ip}:{port}")






import zmq
import msgpack
import cv2
import numpy as np
import threading
import time
import json
import os

# Path to the shared agent file
AGENTS_FILE = "/app/agent_instances.json"

# Configuration
display_window_size = 5
min_ready_frames = 3

frame_conter = 0
fps_timer = time.time()
context = zmq.Context()
frame_buffer = {}
expected_frame_id = 0

# Track running agent threads
started_detectors = {}  # {(ip, port): {"thread": Thread, "stop_event": Event}}


# Load agents from JSON file
def load_agents():
    try:
        with open(AGENTS_FILE, "r") as f:
            data = json.load(f)
            return [(d["ip"], d["port"]) for d in data]
    except Exception as e:
        print(f"[ERROR] Could not read {AGENTS_FILE}: {e}")
        return []


# Receive frames from one detector VM
def receive_from_vm(ip, port, stop_event):
    sock = context.socket(zmq.PULL)
    sock.connect(f"tcp://{ip}:{port}")
    print(f"[INFO] Connected to detector at {ip}:{port}")
    
    while not stop_event.is_set():
        try:
            msg = sock.recv(flags=zmq.NOBLOCK)
            data = msgpack.unpackb(msg, raw=False)
            frame_id = data["frame_id"]
            jpg = data["image"]
            print(f"frame id :{frame_id} with size: {len(msg)} received from VM {ip}")
            frame = cv2.imdecode(np.frombuffer(jpg, np.uint8), cv2.IMREAD_COLOR)
            if frame is not None:
                frame_buffer[frame_id] = frame
        except zmq.Again:
            time.sleep(0.01)
        except Exception as e:
            print(f"[ERROR] {ip}:{port} {e}")
            break

    print(f"[INFO] Stopped receiver for {ip}:{port}")
    sock.close()
    cv2.destroyWindow(f"{ip}:{port}")


# Watch for changes in the agent list and manage threads
def watch_for_new_agents():
    while True:
        agents = load_agents()
        current_set = set(agents)
        known_set = set(started_detectors.keys())

        # Stop removed agents
        removed_agents = known_set - current_set
        for agent in removed_agents:
            print(f"[INFO] Stopping thread for {agent[0]}:{agent[1]}")
            started_detectors[agent]["stop_event"].set()
            started_detectors[agent]["thread"].join()
            del started_detectors[agent]

        # Start new agents
        for ip, port in current_set - known_set:
            print(f"[INFO] Starting thread for {ip}:{port}")
            stop_event = threading.Event()
            thread = threading.Thread(target=receive_from_vm, args=(ip, port, stop_event), daemon=True)
            thread.start()
            started_detectors[(ip, port)] = {"thread": thread, "stop_event": stop_event}

        time.sleep(3)


# Start monitoring thread
threading.Thread(target=watch_for_new_agents, daemon=True).start()

# OpenCV display loop
cv2.namedWindow("YOLOv5 Reconstructed", cv2.WINDOW_NORMAL)
cv2.resizeWindow("YOLOv5 Reconstructed", 960, 540)

try:
    while True:
        if len(frame_buffer) >= min_ready_frames:
            sorted_ids = sorted(frame_buffer.keys())
            to_display = sorted_ids[:display_window_size]
            for fid in to_display:
                frame = frame_buffer.pop(fid)
                cv2.imshow("YOLOv5 Reconstructed", frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    raise KeyboardInterrupt
        time.sleep(0.01)
except KeyboardInterrupt:
    print("\n[SHUTDOWN] Stopping all threads...")
    for agent in list(started_detectors.keys()):
        started_detectors[agent]["stop_event"].set()
    for agent in list(started_detectors.keys()):
        started_detectors[agent]["thread"].join()
    print("[SHUTDOWN] Viewer closed.")

cv2.destroyAllWindows()
